{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fast and frugal memory search for communcation\n",
        "**Authors**: Collin J. Kovacs, Jasper M. Wilson, and Abhilasha A. Kumar\n",
        "\n",
        "> *Communication involves searching for optimal utterances\n",
        "within memory and then evaluating those utterances against\n",
        "a target goal. However, whether this search process converges\n",
        "onto the optimal response relatively quickly, or involves more\n",
        "strategic decision-making to evaluate different candidates re-\n",
        "mains understudied. In this work, speakers generated single\n",
        "word “clues” that would enable a listener to correctly iden-\n",
        "tify a pair of words among several distractor words. Speak-\n",
        "ers and listeners generated candidates before producing final\n",
        "responses. Each player was biased towards the first candi-\n",
        "date(s) they generated, even when this candidate was sub-\n",
        "optimal compared to other candidates. Furthermore, straying\n",
        "away from the initial “patch” of responses decreased accuracy\n",
        "in the game. Overall, these findings suggest that individuals\n",
        "tend to identify the relevant semantic cluster early on during\n",
        "semantic search, and are likely to employ the “take-the-first”\n",
        "strategy in ambiguous semantic contexts.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tINRhKuwC89m"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EyD2S81EC5yq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCmL2HQCDyn0"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nihQIHADDzEH"
      },
      "outputs": [],
      "source": [
        "def create_new_data_path(path:str = \"new_data\") -> str:\n",
        "  \"\"\"Check if the new_data path exists\n",
        "\n",
        "  Args:\n",
        "      path (str, optional): Path to the \"new_data\" folder. Defaults to \"new_data\".\n",
        "\n",
        "  Returns:\n",
        "      str: the path to the new_data folder\n",
        "  \"\"\"\n",
        "  if not os.path.isdir(path):\n",
        "      os.makedirs(path)\n",
        "  return path\n",
        "\n",
        "def cosine_similarity(v1: np.array, v2: np.array) -> float:\n",
        "  \"\"\"Calculate the cosine similarity between two vectors\n",
        "\n",
        "  Args:\n",
        "      v1 (np.array): one vector\n",
        "      v2 (np.array): another vector\n",
        "\n",
        "  Returns:\n",
        "      float: similarity score between the two vectors\n",
        "  \"\"\"\n",
        "  v1 = np.array(v1).flatten()\n",
        "  v2 = np.array(v2).flatten()\n",
        "  return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "def return_emb(word: str, source_emb: pd.DataFrame) -> str or np.NaN:\n",
        "  \"\"\"Return the word that exists in the embedding\n",
        "\n",
        "  Args:\n",
        "      word (str): given word from game\n",
        "      source_emb (pd.DataFrame): embedding dataset\n",
        "\n",
        "  Returns:\n",
        "      str or np.NaN: the filtered word that exists in the embedding dataset\n",
        "  \"\"\"\n",
        "  try:\n",
        "    emb = source_emb[word]\n",
        "    return word\n",
        "  except KeyError:\n",
        "    word_stem = ps.stem(word)\n",
        "    try:\n",
        "      emb = source_emb[word_stem]\n",
        "      return word_stem\n",
        "    except KeyError:\n",
        "      return np.NaN\n",
        "\n",
        "def load_online_data(out_path: str, online_path:str = \"./data/online.csv\") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "  \"\"\"Load the data from the online dataset.\n",
        "\n",
        "  Args:\n",
        "      out_path (str): the path that the upgraded online dataset should be saved to.\n",
        "      online_path (str, optional): the path where the online data exists. Defaults to \"./data/online.csv\".\n",
        "\n",
        "  Returns:\n",
        "      tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: the different versions of the online dataset\n",
        "  \"\"\"\n",
        "  rankings = {'Easy': 1, 'Medium': 2, 'Hard': 3}\n",
        "\n",
        "  online = pd.read_csv(online_path)\n",
        "  online['Ranking'] = online['Level'].apply(lambda x: rankings[x])\n",
        "  online_clues = online.loc[:, ['wordpair_id', 'gameID', 'clueOption1', 'clueOption2', 'clueOption3', 'clueOption4', 'clueOption5', 'clueOption6', 'clueOption7', 'clueOption8', 'clueFinal', 'Acc', 'Level', 'Ranking']]\n",
        "  online_unique_clues = online[['wordpair_id', 'gameID', 'clueOption1', 'clueOption2', 'clueOption3', 'clueOption4', 'clueOption5', 'clueOption6', 'clueOption7', 'clueOption8', 'target1', 'target2']]\n",
        "  online_clues['Number of Clues'] = [len(online_unique_clues.loc[i,:].dropna().to_list()) for i in range(len(online_clues))]\n",
        "  online_guesses = online.loc[:, ['wordpair_id', 'gameID', 'GuessOption1', 'GuessOption2', 'GuessOption3', 'GuessOption4', 'GuessOption5', 'GuessOption6', \n",
        "                                  'GuessOption7', 'GuessOption8', \"target1\", \"target2\"]]\n",
        "  online.to_csv(os.path.join(out_path, \"candidate-generation.csv\"), index = False)\n",
        "  return (online, online_clues, online_unique_clues, online_guesses)\n",
        "\n",
        "def load_embeddings_data(swow_path:str = \"./data/swow_embeddings.csv\") -> pd.DataFrame:\n",
        "  \"\"\"Load the SWOW embedding\n",
        "\n",
        "  Args:\n",
        "      swow_path (str, optional): path to the SWOW embeddings. Defaults to \"./data/swow_embeddings.csv\".\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: the read-in SWOW embeddings\n",
        "  \"\"\"\n",
        "  return pd.read_csv(swow_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_data_path = create_new_data_path()\n",
        "online, online_clues, online_unique_clues, online_guesses = load_online_data(out_path=new_data_path)\n",
        "swow = load_embeddings_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-QmA2MtCw8S"
      },
      "source": [
        "We evaluate whether the responses show signatures of clustering and/or foraging typically found in semantic retrieval tasks. we use a patchy semantic space and ask whether the candidate responses show any evidence of transitions within and outside the patch, and whether these are related to correct responses from the listener. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WgLWjXjsCxiV"
      },
      "outputs": [],
      "source": [
        "class PatchForaging:\n",
        "  def __init__(self, online: pd.DataFrame = online, patch_name:str = \"patch_words.csv\", movement_name: str = \"in_out_transitions.csv\"):\n",
        "    \"\"\"Initialize variables and run methods to create data\n",
        "\n",
        "    Args:\n",
        "        online (pd.DataFrame, optional): online data. Defaults to online.\n",
        "        patch_name (str, optional): name of the patch data file to be saved. Defaults to \"patch_words.csv\".\n",
        "        movement_name (str, optional): name of the movement data file to be saved. Defaults to \"in_out_transitions.csv\".\n",
        "    \"\"\"\n",
        "    self.patch_path = os.path.join(new_data_path, patch_name)\n",
        "    self.movement_path = os.path.join(new_data_path, movement_name)\n",
        "    self.online = online\n",
        "    self.patch = self._patch()\n",
        "    self.movement = self._determine_movement()\n",
        "\n",
        "  def _patch(self) -> pd.DataFrame:\n",
        "    \"\"\"Determine the patch of words for each unique wordpair_id.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A dataframe of the patches with respect to the gameID and wordpair_id.\n",
        "    \"\"\"\n",
        "    if os.path.exists(self.patch_path):\n",
        "      return pd.read_csv(self.patch_path)\n",
        "\n",
        "    wpid = self.online['wordpair_id'].unique()\n",
        "    rows = []\n",
        "    for t in wpid:\n",
        "      temp = self.online[self.online['wordpair_id'] == t].reset_index()\n",
        "      temp = temp.loc[:, ['wordpair_id', 'gameID', 'clueOption1', 'target1', 'target2', 'Level', 'Acc', 'clueFinal']].reset_index(drop = True)\n",
        "      row_frame = temp[['gameID', 'clueFinal', 'Acc', 'Level']]\n",
        "      row_frame['wordpair_id'] = t\n",
        "      row_frame['target1'] = temp.loc[0, 'target1']\n",
        "      row_frame['target2'] = temp.loc[0, 'target2']\n",
        "      clues = list(temp['clueOption1'].unique())\n",
        "      row_frame['words_in_patch'] = ','.join(clues)\n",
        "      row_frame['patchsize'] = len(clues)\n",
        "      row_frame['Level'] = temp.loc[0, 'Level']\n",
        "\n",
        "      rows.append(row_frame)\n",
        "\n",
        "    frame = pd.concat(rows).reset_index(drop = True).rename({'index': 'row_id'}, axis = 1)\n",
        "    frame.to_csv(self.patch_path, index = False)\n",
        "    return frame\n",
        "\n",
        "  def _movement(self, words: list) -> str:\n",
        "    \"\"\"Method to determine the movement from the first word to the second and if it is\n",
        "    inside the patch or not.\n",
        "\n",
        "    Args:\n",
        "        words (list): The two words to be compared for patch movement\n",
        "\n",
        "    Returns:\n",
        "        str: The movement designation.\n",
        "    \"\"\"\n",
        "    first, second = words\n",
        "    if first[1] == 1 and second[1] == 1:\n",
        "      return 'In-In'\n",
        "    elif first[1] == 1 and second[1] == 0:\n",
        "      return 'In-Out'\n",
        "    elif first[1] == 0 and second[1] == 1:\n",
        "      return 'Out-In'\n",
        "    else:\n",
        "      return 'Out-Out'\n",
        "\n",
        "  def _determine_movement(self) -> pd.DataFrame:\n",
        "    \"\"\"Method to determine the movement of the words inside of the patch and how many times each\n",
        "    movement occurs.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A dataframe that contains the counts of the movements given the unique\n",
        "        identifiers.\n",
        "    \"\"\"\n",
        "    if os.path.exists(self.movement_path):\n",
        "      return pd.read_csv(self.movement_path)\n",
        "    \n",
        "    temp_options = self.online[['wordpair_id', 'gameID', 'clueOption1', 'clueOption2', 'clueOption3','clueOption4', 'clueOption5', 'clueOption6', 'clueOption7', 'clueOption8', 'clueFinal', 'Acc']]\n",
        "    temp_options = temp_options.reset_index().rename({'index': 'row_id'}, axis = 1)\n",
        "    rows = []\n",
        "    for i in tqdm(range(len(temp_options))):\n",
        "      r = {}\n",
        "      row = temp_options.loc[i, :].dropna()\n",
        "      r['wordpair_id'] = row.wordpair_id\n",
        "      r['gameID'] = row.gameID\n",
        "      r['In-In'] = 0\n",
        "      r['In-Out'] = 0\n",
        "      r['Out-In'] = 0\n",
        "      r['Out-Out'] = 0\n",
        "\n",
        "      words = list(row.drop(['wordpair_id', 'Acc', 'clueFinal']).to_list())\n",
        "      words = [w for w in words if isinstance(w, str)]\n",
        "      patch_wordpair = self.patch.loc[self.patch['wordpair_id'] == row.wordpair_id, :]\n",
        "      words_in_patch = [(w, 1) if w in patch_wordpair['words_in_patch'].to_list()[0].split(',') else (w, 0) for w in words ]\n",
        "      perms = [(words_in_patch[i], words_in_patch[i+1]) for i in range(0, len(words_in_patch)-1)]\n",
        "      movements = pd.Series([self._movement(p) for p in perms])\n",
        "      ps = pd.DataFrame(movements.value_counts().reset_index()).rename({'index': 'Type', 0: 'Count'}, axis = 1)\n",
        "\n",
        "      ps_d = dict(zip(ps['Type'], ps['Count']))\n",
        "      for k, v in ps_d.items():\n",
        "        r[k] = v\n",
        "      r['Acc'] = row.Acc\n",
        "      r['ClueFinal'] = row.clueFinal\n",
        "      rows.append(r)\n",
        "      \n",
        "    total = pd.DataFrame.from_dict(rows)\n",
        "    total.to_csv(self.movement_path, index = False)\n",
        "    return total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5uuWHK7W47w",
        "outputId": "5ba2fcfe-1773-42cd-aa6e-7bf49ebb3a82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1456/1456 [00:04<00:00, 302.21it/s]\n"
          ]
        }
      ],
      "source": [
        "pf = PatchForaging()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each sequence of candidates generated by a person (lighter-flash-bright-lightning), find out whether they are closer to one word (quick) or another (glow) and assign a 1 or 2 to each candidate response (use swow embeddings for determining semantic similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Predictions:\n",
        "  swow_similarity: pd.DataFrame = None\n",
        "\n",
        "  def __init__(self, \n",
        "               processed_path: str = new_data_path,\n",
        "               clue: pd.DataFrame = online_unique_clues):\n",
        "    \"\"\"Init variables and run the methods to extract the data.\n",
        "\n",
        "    Args:\n",
        "        processed_path (str, optional): path for where the new data should be saved. Defaults to new_data_path.\n",
        "        clue (pd.DataFrame, optional): dataset that contains all unique clues from the experiment. Defaults to online_unique_clues.\n",
        "    \"\"\"\n",
        "    self.processed_path = processed_path\n",
        "    self.clue = clue\n",
        "    self.melted_clue = self._melt_clue()\n",
        "    self.embeddings = self._create_embeddings()\n",
        "\n",
        "    if Predictions.swow_similarity is None:\n",
        "      Predictions.swow_similarity = self.swow_similarity_clustering()\n",
        "\n",
        "  def _melt_clue(self) -> pd.DataFrame:\n",
        "    \"\"\"Melt or pivot longer the clue dataset\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Melted dataframe\n",
        "    \"\"\"\n",
        "    melted_clue = pd.melt(self.clue, id_vars = ['wordpair_id', 'gameID', 'target1', 'target2']).sort_values(['target1', 'target2']).dropna().reset_index(drop = True)\n",
        "    melted_clue['value'] = melted_clue['value'].str.lower()\n",
        "    return melted_clue\n",
        "\n",
        "  def _create_embeddings(self) -> pd.DataFrame:\n",
        "    \"\"\"Extract the embeddings for each target (from wordpair_id) and given clue for eventual similarity measure between each.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe that contains all of the embeddings \n",
        "    \"\"\"\n",
        "    all_words = pd.Series(self.melted_clue['target1'].to_list() + self.melted_clue['target2'].to_list() + self.melted_clue['value'].to_list())\n",
        "    unq_words = list(set(all_words))\n",
        "    rows = []\n",
        "    index_ = []\n",
        "    none_vector = [np.NaN for _ in range(300)]\n",
        "    for word in unq_words:\n",
        "\n",
        "      # collect the embeddings for the targets and the given word\n",
        "      try:\n",
        "        emb = swow[return_emb(word, swow)].values.tolist()\n",
        "      except:\n",
        "        emb = none_vector\n",
        "\n",
        "      rows.append(emb)\n",
        "      index_.append(word)\n",
        "    emb_data = pd.DataFrame(rows, index = index_)\n",
        "    emb_data = emb_data.reset_index().rename({'index': 'value'}, axis = 1)\n",
        "    return emb_data\n",
        "\n",
        "  def swow_similarity_clustering(self) -> pd.DataFrame:\n",
        "    \"\"\"Establish clusters towards either target given the clue through cosine similarity\n",
        "    from each words embeddings.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Predictions of which clue should be clustered into which target cluster\n",
        "    \"\"\"\n",
        "    swow_path = os.path.join(self.processed_path, 'swow_arc.csv')\n",
        "    if os.path.exists(swow_path):\n",
        "      return pd.read_csv(swow_path)\n",
        "\n",
        "    swow_data = self.melted_clue.copy()\n",
        "    closesttarget = []\n",
        "    for i, row in tqdm(self.melted_clue.iterrows()):\n",
        "      target1, target2, word = row.target1, row.target2, row.value\n",
        "      \n",
        "      # extract words and check if the word is None\n",
        "      target1 = self.embeddings[self.embeddings.value == target1].drop('value', axis = 1).values\n",
        "      target2 = self.embeddings[self.embeddings.value == target2].drop('value', axis = 1).values\n",
        "      word = self.embeddings[self.embeddings.value == word].drop('value', axis = 1).values\n",
        "\n",
        "      if word is not None:\n",
        "        sim_target1_word = cosine_similarity(target1, word)\n",
        "        sim_target2_word = cosine_similarity(target2, word)\n",
        "      else:\n",
        "        sim_target1_word = sim_target2_word = 0\n",
        "\n",
        "      # find similarity\n",
        "      if sim_target1_word > sim_target2_word:\n",
        "        closesttarget.append(1)\n",
        "      else:\n",
        "        closesttarget.append(2)\n",
        "\n",
        "    swow_data['Prediction_SWOW'] = closesttarget\n",
        "    swow_data = swow_data.sort_values(['wordpair_id', 'gameID', 'variable']).reset_index(drop = True)\n",
        "    swow_data.to_csv(swow_path, index = False)\n",
        "    return swow_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4821it [00:19, 248.93it/s]\n"
          ]
        }
      ],
      "source": [
        "predictions = Predictions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wordpair_id</th>\n",
              "      <th>gameID</th>\n",
              "      <th>target1</th>\n",
              "      <th>target2</th>\n",
              "      <th>variable</th>\n",
              "      <th>value</th>\n",
              "      <th>Prediction_SWOW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ankle-travel</td>\n",
              "      <td>455383-451866</td>\n",
              "      <td>ankle</td>\n",
              "      <td>travel</td>\n",
              "      <td>clueOption1</td>\n",
              "      <td>run</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ankle-travel</td>\n",
              "      <td>455383-451866</td>\n",
              "      <td>ankle</td>\n",
              "      <td>travel</td>\n",
              "      <td>clueOption2</td>\n",
              "      <td>dance</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ankle-travel</td>\n",
              "      <td>455383-451866</td>\n",
              "      <td>ankle</td>\n",
              "      <td>travel</td>\n",
              "      <td>clueOption3</td>\n",
              "      <td>body</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ankle-travel</td>\n",
              "      <td>455606-490832</td>\n",
              "      <td>ankle</td>\n",
              "      <td>travel</td>\n",
              "      <td>clueOption1</td>\n",
              "      <td>feet</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ankle-travel</td>\n",
              "      <td>455606-490832</td>\n",
              "      <td>ankle</td>\n",
              "      <td>travel</td>\n",
              "      <td>clueOption2</td>\n",
              "      <td>legs</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4816</th>\n",
              "      <td>trauma-weird</td>\n",
              "      <td>490624-466707</td>\n",
              "      <td>trauma</td>\n",
              "      <td>weird</td>\n",
              "      <td>clueOption2</td>\n",
              "      <td>children</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4817</th>\n",
              "      <td>trauma-weird</td>\n",
              "      <td>490624-466707</td>\n",
              "      <td>trauma</td>\n",
              "      <td>weird</td>\n",
              "      <td>clueOption3</td>\n",
              "      <td>outcast</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4818</th>\n",
              "      <td>trauma-weird</td>\n",
              "      <td>491095-486542</td>\n",
              "      <td>trauma</td>\n",
              "      <td>weird</td>\n",
              "      <td>clueOption1</td>\n",
              "      <td>flashbacks</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4819</th>\n",
              "      <td>trauma-weird</td>\n",
              "      <td>491095-486542</td>\n",
              "      <td>trauma</td>\n",
              "      <td>weird</td>\n",
              "      <td>clueOption2</td>\n",
              "      <td>nightmares</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4820</th>\n",
              "      <td>trauma-weird</td>\n",
              "      <td>491095-486542</td>\n",
              "      <td>trauma</td>\n",
              "      <td>weird</td>\n",
              "      <td>clueOption3</td>\n",
              "      <td>dreams</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4821 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       wordpair_id         gameID target1 target2     variable       value  \\\n",
              "0     ankle-travel  455383-451866   ankle  travel  clueOption1         run   \n",
              "1     ankle-travel  455383-451866   ankle  travel  clueOption2       dance   \n",
              "2     ankle-travel  455383-451866   ankle  travel  clueOption3        body   \n",
              "3     ankle-travel  455606-490832   ankle  travel  clueOption1        feet   \n",
              "4     ankle-travel  455606-490832   ankle  travel  clueOption2        legs   \n",
              "...            ...            ...     ...     ...          ...         ...   \n",
              "4816  trauma-weird  490624-466707  trauma   weird  clueOption2    children   \n",
              "4817  trauma-weird  490624-466707  trauma   weird  clueOption3     outcast   \n",
              "4818  trauma-weird  491095-486542  trauma   weird  clueOption1  flashbacks   \n",
              "4819  trauma-weird  491095-486542  trauma   weird  clueOption2  nightmares   \n",
              "4820  trauma-weird  491095-486542  trauma   weird  clueOption3      dreams   \n",
              "\n",
              "      Prediction_SWOW  \n",
              "0                   2  \n",
              "1                   1  \n",
              "2                   1  \n",
              "3                   1  \n",
              "4                   1  \n",
              "...               ...  \n",
              "4816                1  \n",
              "4817                2  \n",
              "4818                2  \n",
              "4819                2  \n",
              "4820                2  \n",
              "\n",
              "[4821 rows x 7 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.swow_similarity"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOVXA20kgbvBp2lLmJdvWMn",
      "include_colab_link": true,
      "name": "paper.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "148bea8a8c99a57e35cbe6f15c85ca52b486fdd4745b8e470a5b94b9ab81b64f"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('CogSci2022-Connector')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
